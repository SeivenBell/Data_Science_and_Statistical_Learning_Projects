{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    mean_squared_log_error,\n",
    "    root_mean_squared_error,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>water</th>\n",
       "      <th>uv</th>\n",
       "      <th>area</th>\n",
       "      <th>fertilizer_usage</th>\n",
       "      <th>yield</th>\n",
       "      <th>pesticides</th>\n",
       "      <th>region</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.072</td>\n",
       "      <td>80.179</td>\n",
       "      <td>9.414</td>\n",
       "      <td>0</td>\n",
       "      <td>29.878</td>\n",
       "      <td>2.231</td>\n",
       "      <td>6</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.413</td>\n",
       "      <td>58.359</td>\n",
       "      <td>9.681</td>\n",
       "      <td>3</td>\n",
       "      <td>53.416</td>\n",
       "      <td>1.810</td>\n",
       "      <td>6</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9.731</td>\n",
       "      <td>78.506</td>\n",
       "      <td>7.189</td>\n",
       "      <td>1</td>\n",
       "      <td>63.391</td>\n",
       "      <td>2.455</td>\n",
       "      <td>1</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.995</td>\n",
       "      <td>69.248</td>\n",
       "      <td>1.738</td>\n",
       "      <td>3</td>\n",
       "      <td>17.984</td>\n",
       "      <td>0.603</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.617</td>\n",
       "      <td>87.658</td>\n",
       "      <td>9.706</td>\n",
       "      <td>1</td>\n",
       "      <td>49.768</td>\n",
       "      <td>2.910</td>\n",
       "      <td>6</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   water      uv   area  fertilizer_usage   yield  pesticides  region  \\\n",
       "0   0   0.072  80.179  9.414                 0  29.878       2.231       6   \n",
       "1   1   5.413  58.359  9.681                 3  53.416       1.810       6   \n",
       "2   2   9.731  78.506  7.189                 1  63.391       2.455       1   \n",
       "3   3  10.995  69.248  1.738                 3  17.984       0.603       2   \n",
       "4   4   2.617  87.658  9.706                 1  49.768       2.910       6   \n",
       "\n",
       "  categories  \n",
       "0          c  \n",
       "1          c  \n",
       "2          d  \n",
       "3          a  \n",
       "4          c  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Google colab link for the report: https://colab.research.google.com/drive/1T9ZKrF6FeLyR1dkLdHS-dyFvR-rxl46F?usp=sharing\n",
    "# url = 'https://github.com/SeivenBell/Data_science_tools/blob/main/Assignment_2/yield_prediction.csv'\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "data_path = \"yield_prediction.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "water               42\n",
       "uv                   0\n",
       "area                 0\n",
       "fertilizer_usage     0\n",
       "yield                0\n",
       "pesticides           0\n",
       "region               0\n",
       "categories           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the id column as it's just an identifier and likely not useful for prediction.\n",
    "\n",
    "# Drop the 'id' column\n",
    "data.drop(\"id\", axis=1, inplace=True)\n",
    "\n",
    "# Check for missing values\n",
    "missing_data = data.isnull().sum()\n",
    "\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 42 missing values in the water column. We'll handle these missing values by imputing them with the median value of the water column, as this is a robust method that is less affected by outliers.\n",
    "\n",
    "After addressing the missing data, we will identify and handle outliers. We'll focus on the numerical features for outlier detection and handling, using the Interquartile Range (IQR) method to identify outliers.\n",
    "https://www.analyticsvidhya.com/blog/2021/05/detecting-and-treating-outliers-treating-the-odd-one-out/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(water               0\n",
       " uv                  0\n",
       " area                0\n",
       " fertilizer_usage    0\n",
       " yield               0\n",
       " pesticides          0\n",
       " region              0\n",
       " categories          0\n",
       " dtype: int64,\n",
       "              water           uv         area  fertilizer_usage        yield  \\\n",
       " count  1000.000000  1000.000000  1000.000000       1000.000000  1000.000000   \n",
       " mean      6.655903    73.943218     8.092453          2.294000    58.595117   \n",
       " std       2.768134     9.996096     2.668264          1.554986    24.073466   \n",
       " min       0.072000    45.320125     0.892125          0.000000     2.843000   \n",
       " 25%       4.695500    66.493000     6.297000          1.000000    40.698000   \n",
       " 50%       6.476000    73.700000     7.987500          3.000000    55.602500   \n",
       " 75%       8.611000    80.608250     9.900250          3.000000    73.645500   \n",
       " max      14.484250   101.781125    15.305125          5.000000   123.066750   \n",
       " \n",
       "         pesticides       region  \n",
       " count  1000.000000  1000.000000  \n",
       " mean      3.452301     3.039000  \n",
       " std       2.076921     1.883886  \n",
       " min       0.014000     0.000000  \n",
       " 25%       1.804500     2.000000  \n",
       " 50%       3.275500     2.000000  \n",
       " 75%       4.916000     5.000000  \n",
       " max       9.532000     6.000000  )"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impute missing values in 'water' column with its median value\n",
    "water_median = data[\"water\"].median()\n",
    "# fillna() function to replace all the null or NaN values in the 'water' column with the calculated median.\n",
    "data[\"water\"].fillna(water_median, inplace=True)\n",
    "\n",
    "\n",
    "# Identify and handle outliers using IQR method for numerical columns\n",
    "numerical_cols = [\"water\", \"uv\", \"area\", \"fertilizer_usage\", \"yield\", \"pesticides\"]\n",
    "\n",
    "for col in numerical_cols:\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define bounds for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Cap the outliers\n",
    "    data[col] = data[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "    # The clip() function is then used to replace all values below the lower bound with the lower\n",
    "    # bound value, and all values above the upper bound with the upper bound value. This is known\n",
    "    # as 'capping' the outliers\n",
    "\n",
    "# Confirm if the missing values are handled\n",
    "missing_data_after = data.isnull().sum()\n",
    "\n",
    "missing_data_after, data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Training Set': 700, 'Validation Set': 150, 'Test Set': 150}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into features and target\n",
    "X = data.drop(\"yield\", axis=1)\n",
    "# creating a new DataFrame X that includes all columns from data except for the \"yield\" column\n",
    "y = data[\"yield\"]\n",
    "\n",
    "# Encode the 'categories' column as it's categorical\n",
    "X = pd.get_dummies(X, columns=[\"categories\"], drop_first=True)\n",
    "\n",
    "# First, split into training and temp (temporary set for further splitting into validation and test sets)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Now, split the temp set equally into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Check the sizes of each set to confirm the split\n",
    "sizes = {\n",
    "    \"Training Set\": len(X_train),\n",
    "    \"Validation Set\": len(X_val),\n",
    "    \"Test Set\": len(X_test),\n",
    "}\n",
    "\n",
    "sizes\n",
    "\n",
    "# Now, use X_train and y_train to train a linear regression model\n",
    "# and X_val, y_val, X_test, and y_test to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Regression Tree Optimization (30 Marks):\n",
    "\n",
    "Apply regression tree and tune parameters to prevent overfitting.\n",
    "Implement post-pruning without Python libraries and compare it with results with pre-pruning and SKlearn post-pruning.\n",
    "Discuss the effectiveness of each method in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 14.407389004951591)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and fit a basic regression tree model\n",
    "basic_tree = DecisionTreeRegressor(random_state=42)\n",
    "basic_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and validation sets\n",
    "y_train_pred_basic = basic_tree.predict(X_train)\n",
    "y_val_pred_basic = basic_tree.predict(X_val)\n",
    "\n",
    "# Calculate and print RMSE for training and validation sets\n",
    "rmse_train_basic = root_mean_squared_error(y_train, y_train_pred_basic)\n",
    "rmse_val_basic = root_mean_squared_error(y_val, y_val_pred_basic)\n",
    "\n",
    "rmse_train_basic, rmse_val_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic regression tree model, without any pruning, resulted in:\n",
    "\n",
    "RMSE (Root Mean Squared Error) on the Training Set: 0.0\n",
    "RMSE on the Validation Set: 14.41\n",
    "The RMSE of 0.0 on the training set indicates that the model has perfectly fitted (overfitted) the training data. However, the RMSE on the validation set is significantly higher, which confirms the overfitting issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Pre-pruning Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12.553251541794502, 15.492205562500928)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Apply pre-pruning techniques\n",
    "pre_pruned_tree = DecisionTreeRegressor(\n",
    "    max_depth=5, min_samples_split=10, min_samples_leaf=6, random_state=42\n",
    ")\n",
    "pre_pruned_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and validation sets\n",
    "y_train_pred_pre_pruned = pre_pruned_tree.predict(X_train)\n",
    "y_val_pred_pre_pruned = pre_pruned_tree.predict(X_val)\n",
    "\n",
    "# Calculate and print RMSE for training and validation sets\n",
    "rmse_train_pre_pruned = root_mean_squared_error(y_train, y_train_pred_pre_pruned)\n",
    "rmse_val_pre_pruned = root_mean_squared_error(y_val, y_val_pred_pre_pruned)\n",
    "\n",
    "rmse_train_pre_pruned, rmse_val_pre_pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results indicate a more balanced model compared to the basic, unpruned tree. The training error has increased (as expected due to the model's reduced complexity), and the validation error is somewhat stable, suggesting reduced overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SKlearn Post-pruning\n",
    "\n",
    "Scikit-learn's post-pruning is implemented through cost complexity pruning. We'll fit a tree with the ccp_alpha parameter, which controls the complexity of the tree and helps to prune it. We will find the optimal ccp_alpha value using cross-validation on the training set and then evaluate the pruned tree on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 1\n",
      "RMSE for training set: 7.999587987621436\n",
      "RMSE for validation set: 14.434149305106823\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Grid search for finding the best ccp_alpha for post-pruning\n",
    "parameters = {\"ccp_alpha\": [0.001, 0.01, 0.1, 1, 10]}\n",
    "tree = DecisionTreeRegressor(random_state=42)\n",
    "clf = GridSearchCV(tree, parameters, scoring=\"neg_mean_squared_error\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Best ccp_alpha value\n",
    "best_ccp_alpha = clf.best_params_[\"ccp_alpha\"]\n",
    "\n",
    "# Train a new tree with the best ccp_alpha\n",
    "pruned_tree = DecisionTreeRegressor(random_state=42, ccp_alpha=best_ccp_alpha)\n",
    "pruned_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and validation sets\n",
    "y_train_pred_pruned = pruned_tree.predict(X_train)\n",
    "y_val_pred_pruned = pruned_tree.predict(X_val)\n",
    "\n",
    "# Calculate and print RMSE for training and validation sets\n",
    "rmse_train_pruned = root_mean_squared_error(y_train, y_train_pred_pruned)\n",
    "rmse_val_pruned = root_mean_squared_error(y_val, y_val_pred_pruned)\n",
    "\n",
    "print(f\"Best ccp_alpha: {best_ccp_alpha}\")\n",
    "print(f\"RMSE for training set: {rmse_train_pruned}\")\n",
    "print(f\"RMSE for validation set: {rmse_val_pruned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying post-pruning with a ccp_alpha of 1, demonstrates a balanced approach to reducing overfitting by pruning the decision tree to a complexity level that maintains a reasonable fit on the training data while attempting to improve generalization on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ccp_alpha: 0.541533938293682\n",
      "RMSE on training set: 6.366274352033018\n",
      "RMSE on validation set: 13.814000019830168\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "\n",
    "# Assuming X_train, X_val, y_train, y_val are already defined\n",
    "\n",
    "# Train a decision tree to full depth to get ccp_alpha values\n",
    "full_tree = DecisionTreeRegressor(random_state=42)\n",
    "full_tree.fit(X_train, y_train)\n",
    "path = full_tree.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "# Remove the maximum ccp_alpha which prunes the tree to a single node\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "# Train decision trees for each ccp_alpha and evaluate on the validation set\n",
    "rmse_val_scores = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    tree = DecisionTreeRegressor(random_state=42, ccp_alpha=ccp_alpha)\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_val_pred = tree.predict(X_val)\n",
    "    rmse_val = root_mean_squared_error(y_val, y_val_pred)  # Corrected this line\n",
    "    rmse_val_scores.append(rmse_val)\n",
    "\n",
    "# Find the best ccp_alpha based on validation RMSE\n",
    "best_alpha = ccp_alphas[np.argmin(rmse_val_scores)]\n",
    "\n",
    "# Train and evaluate a tree using the best ccp_alpha\n",
    "best_tree = DecisionTreeRegressor(random_state=42, ccp_alpha=best_alpha)\n",
    "best_tree.fit(X_train, y_train)\n",
    "y_train_pred = best_tree.predict(X_train)\n",
    "y_val_pred = best_tree.predict(X_val)\n",
    "rmse_train = root_mean_squared_error(y_train, y_train_pred)\n",
    "rmse_val = root_mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Manual Best ccp_alpha: {best_alpha}\")\n",
    "print(f\"Manual RMSE on training set: {rmse_train}\")\n",
    "print(f\"Manual RMSE on validation set: {rmse_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment on optimizing a regression tree to avoid overfitting, we tried out three different methods: pre-pruning, SKlearn's post-pruning, and a manual method for post-pruning.\n",
    "- **Basic, Unpruned Model:**\n",
    "  - Perfectly fit the training data (RMSE = 0.0), but did poorly on the validation set (RMSE = 14.41).\n",
    "  - Clearly overfitted since it memorized the training data too well.\n",
    "\n",
    "- **Pre-pruning Techniques:**\n",
    "  - Made the model simpler by setting limits on the tree's growth.\n",
    "  - Training RMSE increased slightly to 12.55, and validation RMSE to 15.49, showing less overfitting but at the cost of making the model too simple.\n",
    "\n",
    "- **SKlearn Post-pruning:**\n",
    "  - Used SKlearn's tools to find the best complexity parameter, balancing the model better.\n",
    "  - Ended up with a training RMSE of 7.99 and a validation RMSE of 14.43, showing improved balance between fitting the training data and generalizing to new data.\n",
    "\n",
    "- **Manual Post-pruning:**\n",
    "  - Selected the best complexity parameter by manually checking different values.\n",
    "  - Achieved the best balance with a training RMSE of 6.37 and a validation RMSE of 13.81, proving it's a viable and effective method to reduce overfitting while maintaining good model performance.\n",
    "\n",
    "Overall, the manual post-pruning approach provided the best results, demonstrating it's a practical option for fine-tuning a model's complexity to improve how well it generalizes to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use rmse_train and rmse_val for future tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syde_522",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
