{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgGc4qaTjPrU"
      },
      "source": [
        "Welcome to assignment 1.                                                       \n",
        "\n",
        "We are using pathology images for our first assignment please download data from this link https://drive.google.com/drive/folders/10dUOzcPR-PQwfFYcHk5gsLjIjSorQ32Q?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s4K2S-dhTnF"
      },
      "source": [
        "\n",
        "\n",
        "Task 1: Feature Generation (15%)\n",
        "Use and run the following code (a deep network) to generate features from a set of training images. For this assignment, you do not need to know how the deep network is working here to extract features.\n",
        "This code extracts the features of image T4.tif (in the T folder of dataset). Modify the code so that it iterates over all images of the dataset and extracts their features.\n",
        "Allocate 10% of the data for validation.\n",
        "\n",
        "Insert your code here for Task 1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import densenet121\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "#imports for task 2\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "94c_H8Yv7IDs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features and labels for training, testing, and validation sets have been saved.\n"
          ]
        }
      ],
      "source": [
        "# Load the DenseNet model pre-trained on ImageNet\n",
        "model = densenet121(pretrained=True)\n",
        "# Modify the model to remove the last fully connected layer\n",
        "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
        "# Add a global average pooling layer to the model\n",
        "model.add_module(\"global_avg_pool\", torch.nn.AdaptiveAvgPool2d(1))\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Define a series of transformations for preprocessing the images\n",
        "preprocess = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(256),  # Resize the input images to 256x256\n",
        "        transforms.CenterCrop(224),  # Crop the images to 224x224\n",
        "        transforms.ToTensor(),  # Convert the images to PyTorch tensors\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "        ),  # Normalize the images\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Specify the directory containing the dataset\n",
        "dataset_dir = \"train\"\n",
        "\n",
        "# Initialize lists to hold image paths and their corresponding labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through each folder in the dataset directory\n",
        "for folder_name in os.listdir(dataset_dir):\n",
        "    folder_path = os.path.join(dataset_dir, folder_name)\n",
        "    # Check if the path is a directory\n",
        "    if os.path.isdir(folder_path):\n",
        "        # Iterate through each file in the folder\n",
        "        for file_name in os.listdir(folder_path):\n",
        "            # Check if the file is a TIFF image\n",
        "            if file_name.endswith(\".tif\"):\n",
        "                # Append the image path and label to their respective lists\n",
        "                image_paths.append(os.path.join(folder_path, file_name))\n",
        "                labels.append(folder_name)\n",
        "\n",
        "# Convert categorical labels into numeric labels\n",
        "unique_labels = sorted(set(labels))\n",
        "label_to_numeric = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "labels_numeric = [label_to_numeric[label] for label in labels]\n",
        "\n",
        "# Combine image paths and numeric labels into tuples for easy processing\n",
        "combined = list(zip(image_paths, labels_numeric))\n",
        "\n",
        "# Split the combined dataset into training/validation and testing sets\n",
        "train_val_combined, test_combined = train_test_split(\n",
        "    combined, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Further split the training/validation set into separate training and validation sets\n",
        "train_combined, val_combined = train_test_split(\n",
        "    train_val_combined, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "def extract_features_and_labels(combined_data):\n",
        "    \"\"\"\n",
        "    Extract features and labels from the given dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - combined_data: A list of tuples, each containing the path to an image and its numeric label.\n",
        "\n",
        "    Returns:\n",
        "    - A tuple containing two numpy arrays: one for the extracted features and one for the corresponding labels.\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    labels = []\n",
        "    for path, label in combined_data:\n",
        "        # Load the image from the specified path\n",
        "        image = Image.open(path)\n",
        "        # Preprocess the image\n",
        "        input_tensor = preprocess(image)\n",
        "        input_batch = input_tensor.unsqueeze(0)\n",
        "        # Extract features using the model\n",
        "        with torch.no_grad():\n",
        "            output = model(input_batch)\n",
        "        features.append(output.squeeze().detach().numpy())\n",
        "        labels.append(label)\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "\n",
        "# Extract features and labels for training, testing, and validation sets\n",
        "train_features, train_labels = extract_features_and_labels(train_combined)\n",
        "test_features, test_labels = extract_features_and_labels(test_combined)\n",
        "val_features, val_labels = extract_features_and_labels(val_combined)\n",
        "\n",
        "# Save the extracted features and labels to disk\n",
        "np.save(\"train_features.npy\", train_features)\n",
        "np.save(\"test_features.npy\", test_features)\n",
        "np.save(\"val_features.npy\", val_features)\n",
        "np.save(\"train_labels.npy\", train_labels)\n",
        "np.save(\"test_labels.npy\", test_labels)\n",
        "np.save(\"val_labels.npy\", val_labels)\n",
        "\n",
        "print(\"Features and labels for training, testing, and validation sets have been saved.\")\n",
        "\n",
        "# Note on fixing potential warning with updated model loading approach:\n",
        "# Uncomment and use the following code to address deprecation warnings related to loading pretrained models:\n",
        "# from torchvision.models import densenet121, DenseNet121_Weights\n",
        "# model_weights = DenseNet121_Weights.IMAGENET1K_V1  # Alternatively, use DenseNet121_Weights.DEFAULT for the latest weights\n",
        "# model = densenet121(weights=model_weights)\n",
        "# Modify the model similarly as above to prepare for feature extraction\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DguMbSShmHT"
      },
      "source": [
        " Task 2: High Bias Classification Method (5%)\n",
        " Choose a classification method and let is have a high bias.\n",
        " Train it on the generated features and discuss why it is underfitting.\n",
        "\n",
        " Insert your code here for Task 2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "xG7aIh1lhpW3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Fold 1 Accuracy: 1.0\n",
            "Logistic Regression Fold 2 Accuracy: 0.9523809523809523\n",
            "Logistic Regression Fold 3 Accuracy: 0.9841269841269841\n",
            "Logistic Regression Fold 4 Accuracy: 0.9523809523809523\n",
            "Logistic Regression Fold 5 Accuracy: 0.9841269841269841\n",
            "Logistic Regression Fold 6 Accuracy: 0.9841269841269841\n",
            "Logistic Regression Fold 7 Accuracy: 0.9682539682539683\n",
            "Logistic Regression Fold 8 Accuracy: 0.9841269841269841\n",
            "Logistic Regression Fold 9 Accuracy: 0.9523809523809523\n",
            "Logistic Regression Fold 10 Accuracy: 1.0\n",
            "Mean Logistic Regression Accuracy: 0.976190476190476\n",
            "SVM Fold 1 Accuracy: 0.96875\n",
            "SVM Fold 2 Accuracy: 0.9682539682539683\n",
            "SVM Fold 3 Accuracy: 0.9841269841269841\n",
            "SVM Fold 4 Accuracy: 0.9206349206349206\n",
            "SVM Fold 5 Accuracy: 0.9841269841269841\n",
            "SVM Fold 6 Accuracy: 0.9682539682539683\n",
            "SVM Fold 7 Accuracy: 0.9682539682539683\n",
            "SVM Fold 8 Accuracy: 0.9682539682539683\n",
            "SVM Fold 9 Accuracy: 0.9682539682539683\n",
            "SVM Fold 10 Accuracy: 0.9682539682539683\n",
            "Mean SVM Accuracy: 0.9667162698412698\n",
            "SVM Accuracy Variance: 0.00027480798453640733\n"
          ]
        }
      ],
      "source": [
        "# Use a multi-class logistic regression method to classify data\n",
        "# initialize a logistic regression model\n",
        "lr_model = LogisticRegression(max_iter=1000, multi_class=\"ovr\")\n",
        "\n",
        "# perform k-fold cross validation\n",
        "k = 10\n",
        "lr_scores = cross_val_score(lr_model, train_features, train_labels, cv=k)\n",
        "\n",
        "# Print cross-validation scores for logistic regression\n",
        "for i, score in enumerate(lr_scores):\n",
        "    print(f\"Logistic Regression Fold {i+1} Accuracy: {score}\")\n",
        "\n",
        "# Print mean scores for logistic regression\n",
        "mean_lr_accuracy = np.mean(lr_scores)\n",
        "print(f\"Mean Logistic Regression Accuracy: {mean_lr_accuracy}\")\n",
        "\n",
        "# Use a multi-class SVM method to classify data\n",
        "hb_svm_model = svm.SVC(kernel=\"linear\", C=0.01, gamma=1000)\n",
        "hb_svm_scores = cross_val_score(hb_svm_model, train_features, train_labels, cv=k)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "for i, hb_score in enumerate(hb_svm_scores):\n",
        "    print(f\"SVM Fold {i+1} Accuracy: {hb_score}\")\n",
        "\n",
        "# Calculate and print the mean accuracy across all folds\n",
        "hb_mean_accuracy = np.mean(hb_svm_scores)\n",
        "print(f\"Mean SVM Accuracy: {hb_mean_accuracy}\")\n",
        "\n",
        "# Calculate and print the accuracy variance\n",
        "hb_accuracy_var = np.var(hb_svm_scores)  # unterminated string literal\n",
        "print(f\"SVM Accuracy Variance: {hb_accuracy_var}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR8MxxoGhpxF"
      },
      "source": [
        " Task 3: High Variance Classification Method (5%)\n",
        " Use the chosen classification method and let it have a high variance.\n",
        " Train it on the generated features and discuss why it is overfitting.\n",
        "\n",
        " Insert your code here for Task 3\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrsSDN_7huYB"
      },
      "outputs": [],
      "source": [
        "# Use a multi-class SVM method to classify data\n",
        "hv_svm_model = svm.SVC(kernel='sigmoid')\n",
        "hv_svm_scores = cross_val_score(hv_svm_model, train_features, train_labels, cv=k)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "for i, hv_score in enumerate(hv_svm_scores):\n",
        "    print(f\"SVM Fold {i+1} Accuracy: {hv_score}\")\n",
        "\n",
        "# Calculate and print the mean accuracy across all folds\n",
        "hv_mean_accuracy = np.mean(hv_svm_scores)\n",
        "print(f\"Mean SVM Accuracy: {hv_mean_accuracy}\")\n",
        "\n",
        "# Calculate and print the accuracy variance across all folds\n",
        "hv_accuracy_var = np.var(hv_svm_scores)\n",
        "print(f\"SVM Accuracy Variance: {hv_accuracy_var}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzxSVPWXht-m"
      },
      "source": [
        " Task 4: Balanced Classification Method (15%)\n",
        " Use the chosen classification method and let it balance the bias and variance.\n",
        " Train it on the generated features, possibly adjusting parameters.\n",
        " Discuss insights into achieving balance.\n",
        "\n",
        " Insert your code here for Task 4\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjgmSxk7h7vZ"
      },
      "outputs": [],
      "source": [
        "balanced_svm_model = svm.SVC(kernel='linear')\n",
        "balanced_svm_scores = cross_val_score(balanced_svm_model, train_features, \n",
        "                                      train_labels, cv=k)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "for i, balanced_score in enumerate(balanced_svm_scores):\n",
        "    print(f\"SVM Fold {i+1} Accuracy: {balanced_score}\")\n",
        "\n",
        "# Calculate and print the mean accuracy across all folds\n",
        "balanced_mean_accuracy = np.mean(balanced_svm_scores)\n",
        "print(f\"Balanced SVM Mean Accuracy: {balanced_mean_accuracy}\")\n",
        "\n",
        "# Calculate and print the accuracy variance across all folds\n",
        "balanced_accuracy_var = np.var(balanced_svm_scores)\n",
        "print(f\"Balanced SVM Accuracy Variance: {balanced_accuracy_var}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKRG3PfFh8Ot"
      },
      "source": [
        " Task 5: K-Means Clustering (20%)\n",
        " Apply K-Means clustering on the generated features.\n",
        " Test with available labels and report accuracy.\n",
        " Experiment with automated K and compare with manually set 20 clusters.\n",
        "\n",
        " Insert your code here for Task 5\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VLuOkJyAh-mN"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_features' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[18], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming 'train_features' are the features extracted from the images\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Apply K-Means with manually set 20 clusters\u001b[39;00m\n\u001b[0;32m      7\u001b[0m kmeans_20 \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m clusters_20 \u001b[38;5;241m=\u001b[39m kmeans_20\u001b[38;5;241m.\u001b[39mfit_predict(\u001b[43mtrain_features\u001b[49m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Since KMeans does not inherently provide labels matching to original labels,\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# a mapping function or strategy is needed to evaluate clustering accuracy.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Accuracy reporting would require a strategy to match cluster labels to true labels, which is non-trivial\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# because clustering is unsupervised and does not directly produce class labels that match with the ground truth.\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'train_features' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# Assuming 'train_features' are the features extracted from the images\n",
        "\n",
        "# Apply K-Means with manually set 20 clusters\n",
        "kmeans_20 = KMeans(n_clusters=20, random_state=42)\n",
        "clusters_20 = kmeans_20.fit_predict(train_features)\n",
        "\n",
        "# Since KMeans does not inherently provide labels matching to original labels,\n",
        "# a mapping function or strategy is needed to evaluate clustering accuracy.\n",
        "\n",
        "# Apply K-Means with automated K (e.g., using the Elbow method or other heuristic)\n",
        "# This step would involve determining the optimal number of clusters K,\n",
        "# which can be done using methods like the Elbow Method or the Silhouette Score.\n",
        "\n",
        "# Example for Elbow Method (commented out because it requires plotting)\n",
        "\n",
        "\n",
        "# wcss = []\n",
        "# for i in range(1, 21):\n",
        "#     kmeans = KMeans(n_clusters=i, random_state=42)\n",
        "#     kmeans.fit(train_features)\n",
        "#     wcss.append(kmeans.inertia_)\n",
        "# plt.plot(range(1, 21), wcss)\n",
        "# plt.title('Elbow Method')\n",
        "# plt.xlabel('Number of clusters')\n",
        "# plt.ylabel('WCSS')  # Within cluster sum of squares\n",
        "# plt.show()\n",
        "\n",
        "# Assuming optimal K is found, e.g., k_optimal\n",
        "# kmeans_opt = KMeans(n_clusters=k_optimal, random_state=42)\n",
        "# clusters_opt = kmeans_opt.fit_predict(train_features)\n",
        "\n",
        "# Accuracy reporting would require a strategy to match cluster labels to true labels, which is non-trivial\n",
        "# because clustering is unsupervised and does not directly produce class labels that match with the ground truth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43sPfI7Jh-9p"
      },
      "source": [
        " Task 6: Additional Clustering Algorithm (10%)\n",
        " Choose another clustering algorithm and apply it on the features.\n",
        " Test accuracy with available labels.\n",
        "\n",
        " Insert your code here for Task 6\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nn9f41LWiCDr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Assuming 'train_features' are the features extracted from the images\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "dbscan_clusters = dbscan.fit_predict(train_features)\n",
        "\n",
        "# The silhouette score can be used to evaluate the quality of the clustering\n",
        "silhouette_avg = silhouette_score(train_features, dbscan_clusters)\n",
        "print(f\"Silhouette Score: {silhouette_avg}\")\n",
        "\n",
        "# DBSCAN labels outliers as -1, so you might want to handle them in your accuracy assessment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fPfoBsaiCXu"
      },
      "source": [
        " Task 7: PCA for Classification Improvement (20%)\n",
        " Apply PCA on the features and then feed them to the best classification method in the above tasks.\n",
        " Assess if PCA improves outcomes and discuss the results.\n",
        "\n",
        " Insert your code here for Task 7\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoOFXhdmiHeD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQqNra7eiHx-"
      },
      "source": [
        " Task 8: Visualization and Analysis (10%)\n",
        " Plot the features in a lower dimension using dimentinality reduction techniques.\n",
        " Analyze the visual representation, identifying patterns or insights.\n",
        "\n",
        "Insert your code here for Task 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1npTL_NkjNdL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "syde_522",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
