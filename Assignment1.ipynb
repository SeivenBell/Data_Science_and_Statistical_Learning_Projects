{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgGc4qaTjPrU"
      },
      "source": [
        "Welcome to assignment 1.                                                       \n",
        "\n",
        "We are using pathology images for our first assignment please download data from this link https://drive.google.com/drive/folders/10dUOzcPR-PQwfFYcHk5gsLjIjSorQ32Q?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s4K2S-dhTnF"
      },
      "source": [
        "\n",
        "\n",
        "Task 1: Feature Generation (15%)\n",
        "Use and run the following code (a deep network) to generate features from a set of training images. For this assignment, you do not need to know how the deep network is working here to extract features.\n",
        "This code extracts the features of image T4.tif (in the T folder of dataset). Modify the code so that it iterates over all images of the dataset and extracts their features.\n",
        "Allocate 10% of the data for validation.\n",
        "\n",
        "Insert your code here for Task 1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import densenet121\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "94c_H8Yv7IDs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature vector shape: (1024,)\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "extract_features() missing 2 required positional arguments: 'model' and 'preprocess'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Load each .tif file\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m tif_files:\n\u001b[1;32m---> 76\u001b[0m     image, feature_vector \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# Append the label (folder name) to the list\u001b[39;00m\n",
            "\u001b[1;31mTypeError\u001b[0m: extract_features() missing 2 required positional arguments: 'model' and 'preprocess'"
          ]
        }
      ],
      "source": [
        "# Load pre-trained DenseNet model\n",
        "model = densenet121(pretrained=True)\n",
        "\n",
        "# Remove the classification layer (last fully connected layer)\n",
        "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
        "\n",
        "# Add a global average pooling layer\n",
        "model.add_module(\"global_avg_pool\", torch.nn.AdaptiveAvgPool2d(1))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Define the image preprocessing pipeline\n",
        "\n",
        "preprocess = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# # Load an image\n",
        "# image_path = \"train\\T\\T4.tif\"\n",
        "# image = Image.open(image_path)\n",
        "# Preprocess the image\n",
        "input_tensor = preprocess(image)\n",
        "\n",
        "input_batch = input_tensor.unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "# Wrap the input tensor in a Variable\n",
        "\n",
        "input_var = Variable(input_batch)\n",
        "\n",
        "# Forward pass through the model\n",
        "\n",
        "features = model(input_var)\n",
        "\n",
        "# Extract the feature tensor\n",
        "\n",
        "feature_vector = features.squeeze().detach().numpy()\n",
        "\n",
        "# Now 'feature_vector' contains the feature from the last fully connected layer of DenseNet\n",
        "\n",
        "print(\"Feature vector shape:\", feature_vector.shape)\n",
        "\n",
        "# Directory containing the data folder\n",
        "\n",
        "data_dir = \"train\"\n",
        "\n",
        "# List all directories\n",
        "\n",
        "directories = [\n",
        "    folder\n",
        "    for folder in os.listdir(data_dir)\n",
        "    if os.path.isdir(os.path.join(data_dir, folder))\n",
        "]\n",
        "\n",
        "# Initialize lists for storing images and labels\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "features = []\n",
        "\n",
        "for directory in directories:\n",
        "    folder_path = os.path.join(data_dir, directory)\n",
        "    if os.path.isdir(folder_path):\n",
        "        # List all files in the directory\n",
        "        files = os.listdir(folder_path)\n",
        "        # Filter out .tif files\n",
        "        tif_files = [file for file in files if file.endswith(\".tif\")]\n",
        "        # Load each .tif file\n",
        "        for file in tif_files:\n",
        "            image, feature_vector = extract_features(os.path.join(folder_path, file))\n",
        "            images.append(image)\n",
        "\n",
        "            # Append the label (folder name) to the list\n",
        "            labels.append(directory)\n",
        "            # Append feature vector\n",
        "            features.append(feature_vector)\n",
        "\n",
        "\n",
        "# Split paths and labels into train and validation sets\n",
        "\n",
        "train_features, val_features, train_labels, val_labels = train_test_split(\n",
        "    features, labels, test_size=0.1, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DguMbSShmHT"
      },
      "source": [
        " Task 2: High Bias Classification Method (5%)\n",
        " Choose a classification method and let is have a high bias.\n",
        " Train it on the generated features and discuss why it is underfitting.\n",
        "\n",
        " Insert your code here for Task 2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xG7aIh1lhpW3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_features' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# perform k-fold cross validation\u001b[39;00m\n\u001b[0;32m     11\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m---> 12\u001b[0m lr_scores \u001b[38;5;241m=\u001b[39m cross_val_score(lr_model, \u001b[43mtrain_features\u001b[49m, train_labels, cv\u001b[38;5;241m=\u001b[39mk)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Print cross-validation scores for logistic regression\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lr_scores):\n",
            "\u001b[1;31mNameError\u001b[0m: name 'train_features' is not defined"
          ]
        }
      ],
      "source": [
        "# Use a multi-class logistic regression method to classify data\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# initialize a logistic regression model\n",
        "lr_model = LogisticRegression(max_iter=1000, multi_class=\"ovr\")\n",
        "\n",
        "# perform k-fold cross validation\n",
        "k = 10\n",
        "lr_scores = cross_val_score(lr_model, train_features, train_labels, cv=k)\n",
        "\n",
        "# Print cross-validation scores for logistic regression\n",
        "for i, score in enumerate(lr_scores):\n",
        "    print(f\"Logistic Regression Fold {i+1} Accuracy: {score}\")\n",
        "\n",
        "# Print mean scores for logistic regression\n",
        "mean_lr_accuracy = np.mean(lr_scores)\n",
        "print(f\"Mean Logistic Regression Accuracy: {mean_lr_accuracy}\")\n",
        "\n",
        "# Use a multi-class SVM method to classify data\n",
        "hb_svm_model = svm.SVC(kernel=\"linear\", C=0.01, gamma=1000)\n",
        "hb_svm_scores = cross_val_score(hb_svm_model, train_features, train_labels, cv=k)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "for i, hb_score in enumerate(hb_svm_scores):\n",
        "    print(f\"SVM Fold {i+1} Accuracy: {hb_score}\")\n",
        "\n",
        "# Calculate and print the mean accuracy across all folds\n",
        "hb_mean_accuracy = np.mean(hb_svm_scores)\n",
        "print(f\"Mean SVM Accuracy: {hb_mean_accuracy}\")\n",
        "\n",
        "# Calculate and print the accuracy variance\n",
        "hb_accuracy_var = np.var(hb_svm_scores)  # unterminated string literal\n",
        "print(f\"SVM Accuracy Variance: {hb_accuracy_var}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR8MxxoGhpxF"
      },
      "source": [
        " Task 3: High Variance Classification Method (5%)\n",
        " Use the chosen classification method and let it have a high variance.\n",
        " Train it on the generated features and discuss why it is overfitting.\n",
        "\n",
        " Insert your code here for Task 3\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrsSDN_7huYB"
      },
      "outputs": [],
      "source": [
        "# Use a multi-class SVM method to classify data\n",
        "hv_svm_model = svm.SVC(kernel='sigmoid')\n",
        "hv_svm_scores = cross_val_score(hv_svm_model, train_features, train_labels, cv=k)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "for i, hv_score in enumerate(hv_svm_scores):\n",
        "    print(f\"SVM Fold {i+1} Accuracy: {hv_score}\")\n",
        "\n",
        "# Calculate and print the mean accuracy across all folds\n",
        "hv_mean_accuracy = np.mean(hv_svm_scores)\n",
        "print(f\"Mean SVM Accuracy: {hv_mean_accuracy}\")\n",
        "\n",
        "# Calculate and print the accuracy variance across all folds\n",
        "hv_accuracy_var = np.var(hv_svm_scores)\n",
        "print(f\"SVM Accuracy Variance: {hv_accuracy_var}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzxSVPWXht-m"
      },
      "source": [
        " Task 4: Balanced Classification Method (15%)\n",
        " Use the chosen classification method and let it balance the bias and variance.\n",
        " Train it on the generated features, possibly adjusting parameters.\n",
        " Discuss insights into achieving balance.\n",
        "\n",
        " Insert your code here for Task 4\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjgmSxk7h7vZ"
      },
      "outputs": [],
      "source": [
        "balanced_svm_model = svm.SVC(kernel='linear')\n",
        "balanced_svm_scores = cross_val_score(balanced_svm_model, train_features, \n",
        "                                      train_labels, cv=k)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "for i, balanced_score in enumerate(balanced_svm_scores):\n",
        "    print(f\"SVM Fold {i+1} Accuracy: {balanced_score}\")\n",
        "\n",
        "# Calculate and print the mean accuracy across all folds\n",
        "balanced_mean_accuracy = np.mean(balanced_svm_scores)\n",
        "print(f\"Balanced SVM Mean Accuracy: {balanced_mean_accuracy}\")\n",
        "\n",
        "# Calculate and print the accuracy variance across all folds\n",
        "balanced_accuracy_var = np.var(balanced_svm_scores)\n",
        "print(f\"Balanced SVM Accuracy Variance: {balanced_accuracy_var}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKRG3PfFh8Ot"
      },
      "source": [
        " Task 5: K-Means Clustering (20%)\n",
        " Apply K-Means clustering on the generated features.\n",
        " Test with available labels and report accuracy.\n",
        " Experiment with automated K and compare with manually set 20 clusters.\n",
        "\n",
        " Insert your code here for Task 5\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VLuOkJyAh-mN"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_features' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[18], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming 'train_features' are the features extracted from the images\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Apply K-Means with manually set 20 clusters\u001b[39;00m\n\u001b[0;32m      7\u001b[0m kmeans_20 \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m clusters_20 \u001b[38;5;241m=\u001b[39m kmeans_20\u001b[38;5;241m.\u001b[39mfit_predict(\u001b[43mtrain_features\u001b[49m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Since KMeans does not inherently provide labels matching to original labels,\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# a mapping function or strategy is needed to evaluate clustering accuracy.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Accuracy reporting would require a strategy to match cluster labels to true labels, which is non-trivial\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# because clustering is unsupervised and does not directly produce class labels that match with the ground truth.\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'train_features' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# Assuming 'train_features' are the features extracted from the images\n",
        "\n",
        "# Apply K-Means with manually set 20 clusters\n",
        "kmeans_20 = KMeans(n_clusters=20, random_state=42)\n",
        "clusters_20 = kmeans_20.fit_predict(train_features)\n",
        "\n",
        "# Since KMeans does not inherently provide labels matching to original labels,\n",
        "# a mapping function or strategy is needed to evaluate clustering accuracy.\n",
        "\n",
        "# Apply K-Means with automated K (e.g., using the Elbow method or other heuristic)\n",
        "# This step would involve determining the optimal number of clusters K,\n",
        "# which can be done using methods like the Elbow Method or the Silhouette Score.\n",
        "\n",
        "# Example for Elbow Method (commented out because it requires plotting)\n",
        "\n",
        "\n",
        "# wcss = []\n",
        "# for i in range(1, 21):\n",
        "#     kmeans = KMeans(n_clusters=i, random_state=42)\n",
        "#     kmeans.fit(train_features)\n",
        "#     wcss.append(kmeans.inertia_)\n",
        "# plt.plot(range(1, 21), wcss)\n",
        "# plt.title('Elbow Method')\n",
        "# plt.xlabel('Number of clusters')\n",
        "# plt.ylabel('WCSS')  # Within cluster sum of squares\n",
        "# plt.show()\n",
        "\n",
        "# Assuming optimal K is found, e.g., k_optimal\n",
        "# kmeans_opt = KMeans(n_clusters=k_optimal, random_state=42)\n",
        "# clusters_opt = kmeans_opt.fit_predict(train_features)\n",
        "\n",
        "# Accuracy reporting would require a strategy to match cluster labels to true labels, which is non-trivial\n",
        "# because clustering is unsupervised and does not directly produce class labels that match with the ground truth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43sPfI7Jh-9p"
      },
      "source": [
        " Task 6: Additional Clustering Algorithm (10%)\n",
        " Choose another clustering algorithm and apply it on the features.\n",
        " Test accuracy with available labels.\n",
        "\n",
        " Insert your code here for Task 6\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nn9f41LWiCDr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Assuming 'train_features' are the features extracted from the images\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "dbscan_clusters = dbscan.fit_predict(train_features)\n",
        "\n",
        "# The silhouette score can be used to evaluate the quality of the clustering\n",
        "silhouette_avg = silhouette_score(train_features, dbscan_clusters)\n",
        "print(f\"Silhouette Score: {silhouette_avg}\")\n",
        "\n",
        "# DBSCAN labels outliers as -1, so you might want to handle them in your accuracy assessment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fPfoBsaiCXu"
      },
      "source": [
        " Task 7: PCA for Classification Improvement (20%)\n",
        " Apply PCA on the features and then feed them to the best classification method in the above tasks.\n",
        " Assess if PCA improves outcomes and discuss the results.\n",
        "\n",
        " Insert your code here for Task 7\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoOFXhdmiHeD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQqNra7eiHx-"
      },
      "source": [
        " Task 8: Visualization and Analysis (10%)\n",
        " Plot the features in a lower dimension using dimentinality reduction techniques.\n",
        " Analyze the visual representation, identifying patterns or insights.\n",
        "\n",
        "Insert your code here for Task 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1npTL_NkjNdL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "syde_522",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
